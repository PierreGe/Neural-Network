\documentclass[a4paper,10pt]{article}
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[left=2.5cm,top=2cm,right=2.5cm,nohead,nofoot]{geometry}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{amsmath}

\linespread{1.1}



\begin{document}

\begin{titlepage}
\begin{center}
\textbf{\textsc{UNIVERSIT\'E DE MONTR\'EAL}}\\
%\textbf{\textsc{Faculté des Sciences}}\\
%\textbf{\textsc{Département d'Informatique}}
\vfill{}\vfill{}
\begin{center}{\Huge Rapport : Devoir2 }\end{center}{\Huge \par}
\begin{center}{\large Pierre Gérard \\ Mathieu Bouchard}\end{center}{\Huge \par}
\vfill{}\vfill{} \vfill{}
\begin{center}{\large \textbf{IFT3395-6390 Fondements de l'apprentissage machine}}\hfill{\\Pascal Vincent, Alexandre de Brébisson et César Laurent}\end{center}{\large\par}
\vfill{}\vfill{}\enlargethispage{3cm}
\textbf{Année académique 2015~-~2016}
\end{center}
\end{titlepage}

%\begin{abstract}
%Ce rapport présente ...
%\end{abstract}


%\tableofcontents

%\pagebreak

\section{Partie théorique: Calcul du gradient pour l’optimisation des paramètres d'un réseau de neurones}

Commençons par dessiner un rapide schéma du réseau de neurones étudié.

\begin{figure}[H]
	\includegraphics[width=12cm]{reseau.jpg}
	\centering
	\label{fig:comp}
\end{figure}

\subsection{Exercice a)}

b est de dimension $d_{h}$

Le vecteur d'activation est : $h_{a} = W^{(1)}x +b$

Avec $ h_{a_i} = W^{(1)}_{i1} x_{1}  W^{(1)}_{i2} x_{2} + ... + W^{(1)}_{id} x_{d} b_{i}$

Et $h_{s_i} = h_{a_i}*I_{ \{ h_{a_i} > 0 \} } = max(h_{a_i}, 0)$

\subsection{Exercice b)}

$W^{(2)}$ est de dimension $m \times d_{h}$

$b^{(2)}$ est de dimension $m$

Le vecteur d'activation est : $o^{a} = W^{(2)} h_{s} + b^{(2)}$

Avec $o^{a}_{k} = W^{(2)}_{k1} h_{s_1} + W^{(2)}_{k2} h_{s_2} + ... + W^{(2)}_{kn} h_{s_n} + b^{(2)}_{k}$

\subsection{Exercice c)}

$o^{s} = softmax(o^{a}) = \frac{1}{\sum_{i=1}^{m} e^{o^{a}_{i}}}  (e^{o^{a}_{1}}, e^{o^{a}_{2}}, ..., e^{o^{a}_{n}})$

Donc $o^{s}_{k} = \frac{e^{o^{a}_{k}}}{\sum_{i=1}^{m} e^{o^{a}_{i}}}$

$e^{x} : \mathds{R} \rightarrow \mathds{R}^{+}$ donc la somme au numérateur de la fonction ci-dessus sera positive et la somme au numérateur aussi. Une fraction de deux nombres positifs sera toujours positif donc $o^{s}_{k}$ est toujours positif.

$\sum^{m}_{i=1} o^{s}_{i} = \sum^{m}_{i=1} \frac{e^{o^{a}_{i}}}{\sum_{j=1}^{m} e^{o^{a}_{j}}}$

$ = \frac{1}{\sum_{j=1}^{m} e^{o^{a}_{j}}} \sum^{m}_{i=1} e^{o^{a}_{i}}$

$ = \frac{\sum^{m}_{i=1} e^{o^{a}_{i}}}{\sum_{j=1}^{m} e^{o^{a}_{j}}}$

$ =1 $


C'est important car cela signifie que les sorties sont les probabilité pour l'entrée d'être d'une certaine classe et ces classes sont mutuellement exclusives.

\subsection{Exercice d)}

$ L(x,y) = -log  (o^{s}_{y}(x)) $

$= -log \frac{e^{o^{a}_{y}(x)}}{\sum_{i=1}^{m} e^{o^{a}_{i}(x)}}$ 

$= -log(e^{o^{a}_{y}(x)}) +log(\sum_{i=1}^{m} e^{o^{a}_{o}(x)})  $

$ = -o^{a}_{y}(x) + log(\sum_{i=1}^{m} e^{o^{a}_{i}(x)})$

\subsection{Exercice e)} 

L'erreur empirique vaut :

$\widehat{R}(f,D) = \sum_{i=1}^{n} L(x,y)$

\todo{demander en tp des précisions sur la notation ci-dessus}

Les paramètres sont :

$ \theta = \{ W^{(1)},W^{(2)},b^{(1)},b^{(2)} \}$ avec $W^{(1)}$ et $b^{(1)}$ représentant les connexions synaptiques entre l'entrée et la couche cachée et $W^{(2)}$ et $b^{(2)}$ représentant les connexions synaptiques entre la couche cachée et la sortie.

Le problèmes d'optimisation revient donc à l'équation suivante :

$\theta^{*} = argmin_{\theta} \widehat{R}(f,D) $

Les dimensions sont :

$n_{\theta}$ est de dimension $d_{h} \times d + d_{h} + m \times d_{h} + m$

\subsection{Exercice f)} 

\begin{verbatim}
	def gradient(ensemble_donne):
.		somme = 0
.		for x in ensemble_donne:
.		.	sum += derivation_lost(x)
.		return somme
\end{verbatim}

\begin{verbatim}
	theta = initialisation des params de maniere random
	epsilon =
	learningRate =
	while learningRate*gradient() < epsilon :  # attention aux boucles infini
.		theta = theta + learningRate*gradient()
\end{verbatim}
\subsection{Exercice g)}

\subsection{Exercice h)}

\begin{verbatim}
	import numpy as np
	grad_oa = 1/np.sum(e_oa) * np.array(e_oa)
	grad_oa[y] = grad_oa[y] - 1  #onehot
\end{verbatim}

\subsection{Exercice i)}
Réponse entière donnée

\subsection{Exercice j)} 

La dimension de :
\begin{itemize}
	\item $\frac{\partial L}{\partial b^{(2)}}$ est $m$
	\item $\frac{\partial L}{\partial W^{(2)}}$ est $ m \times d_{h}$
	\item $\frac{\partial L}{\partial o^{a}}$ est $ m \times 1$
	\item $h^{s^T}$ est $ 1 \times d_{h}$
\end{itemize}

\begin{verbatim}
	grad_b2 = grad_oa
	grad_w2 = grad_oa * np.transpose(h_s)
\end{verbatim}

\subsection{Exercice k)}
Réponse entière donnée

\subsection{Exercice l)}

La dimension de :
\begin{itemize}
	\item $\frac{\partial L}{\partial h^{s}}$ est $d_{h}$
	\item $W^{(2)^T} $ est $d_{h} \times m$
	\item $\frac{\partial L}{\partial o^{a}}$ est $ m \times 1$
\end{itemize}

\begin{verbatim}
 grad_hs = np.transpose(w_2) * grad_oa	
\end{verbatim}

\subsection{Exercice m)}

\subsection{Exercice n)}

\begin{verbatim}
	(a + np.abs(a))/2  # astuce pour garder que les nombre positif
\end{verbatim}


\subsection{Exercice o)}

Pour $b^{(1)}$ :

$\frac{\partial L}{\partial b^{(1)}_{k}} = \frac{\partial L}{\partial h^{a}_{k}} \frac{\partial h^{a}_{k}}{\partial b^{(1)}_{k}}$

$ = \frac{\partial L}{\partial h^{a}_{k}} \frac{\partial \sum_{j'} W^{(1)}_{kj'} x_{j} + b^{(1)}_{k}}{\partial b^{1}_{k}}$

$ = \frac{\partial L}{\partial h^{a}_{k}}$

Pour $W^{(1)}$ :

$\frac{\partial L}{\partial W^{(1)}_{kj}} = \frac{\partial L}{\partial h^{a}_{k}} \frac{\partial h^{a}_{k}}{\partial W^{(1)}_{kj}} $

$ = \frac{\partial L}{\partial h^{a}_{k}} \frac{\partial \sum_{j'} W^{(1)}_{kj'} x_{j} + b^{(1)}_{k}}{\partial W^{(1)}_{kj}}$

$ = \frac{\partial L}{\partial h^{a}_{k}} x_{j} $


\subsection{Exercice p)}

La dimension de :
\begin{itemize}
	\item $\frac{\partial L}{\partial b^{1}}$ est $d_{h}$
	\item $\frac{\partial L}{\partial W^{(1)}}$ est $d_{h} \times n$ car  $\frac{\partial L}{\partial h^{(a)}}$ est $d_{h} \times 1$ et $ x $ est $1 \times n$
\end{itemize}


\subsection{Exercice q)}

\subsection{Exercice r)}

\end{document}
